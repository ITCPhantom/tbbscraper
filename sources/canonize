#! /usr/bin/python
# canonize - canonicalize and clean a list of URLs.  -*- encoding: utf-8 -*-
# Copyright © 2010, 2013, 2014 Zack Weinberg
# Portions © 2009 Serge Broslavsky
#
# Copying and distribution of this program, with or without modification,
# are permitted in any medium without royalty provided the copyright
# notice and this notice are preserved.  This program is offered as-is,
# without any warranty.

import multiprocessing
import optparse
import os
import signal
import socket
import sqlite3
import string
import sys

import cookielib
import httplib
import urllib
import urlparse

# We have to do the HTTP queries by hand because of urllib bugs and
# sites that misbehave if they see its default user-agent.  This means
# we have to create a shim between httplib and cookielib, because
# cookielib assumes you're using urllib.  Thanks to Serge Broslavsky
# for this shim class:
# http://stackoverflow.com/questions/1016765/how-to-use-cookielib-with-httplib-in-python

class HTTPRequest(object):
    """
    Data container for HTTP request (used for cookie processing).
    """

    def __init__(self, url, headers={}, method='GET'):
        self._url = urlparse.urlsplit(urlparse.urldefrag(url)[0])
        # Mimic the headers that would be produced by a real browser.
        self._headers = {
          "User-Agent":
            "Mozilla/5.0 (Macintosh; rv:24.0) Gecko/20100101 Firefox/24.0",
          "Accept":
            "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }
        self._method = method

        for key, value in headers.items():
            self.add_header(key, value)

    def has_header(self, name):
        return name in self._headers

    def add_header(self, key, val):
        self._headers[key.capitalize()] = val

    def add_unredirected_header(self, key, val):
        self._headers[key.capitalize()] = val

    def is_unverifiable(self):
        return True

    def get_type(self):
        return self._url.scheme

    def get_full_url(self):
        return self._url.geturl()

    def get_header(self, header_name, default=None):
        return self._headers.get(header_name.capitalize(), default)

    def get_host(self):
        return self._url.netloc

    get_origin_req_host = get_host

    def get_headers(self):
        return self._headers

    def fire(self):
        if self._url.scheme == 'http':
            conn = httplib.HTTPConnection(self._url.netloc, timeout=10)
        elif self._url.scheme == 'https':
            conn = httplib.HTTPSConnection(self._url.netloc, timeout=10)
        else:
            raise IOError("unsupported URL '%s'" % self.get_full_url())
        path = self._url.path
        if self._url.query != "":
            path = path + '?' + self._url.query

        conn.request(self._method, path, headers=self._headers)
        resp = conn.getresponse()
        # patch httplib response to look like urllib2 response,
        # for the sake of cookie processing
        resp.info = lambda: resp.msg
        return resp

#
# Logging.  Note that sys.stderr is shared among all worker processes
# so we must take care to always print complete lines in a single
# write() call.  The obsessive flushing may or may not be necessary
# depending on exactly which Python version you have.
#

options = None
mypid   = None

def fmt_status(resp):
    return str(resp.status) + " " + str(resp.reason)

def fmt_cookies(jar):
    if not options.verbose: return ""
    if not jar: return ""
    return " [" + " ".join(cookie.name + "=" + cookie.value
                           for cookie in jar) + "]"

def log_start(orig_url):
    if not options.verbose: return
    sys.stderr.write("{:05}: {} ...\n".format(mypid, orig_url))
    sys.stderr.flush()

def log_success(orig_url, canon, resp):
    if not options.verbose: return
    if resp.status != 200:
        sys.stderr.write("{:05}: {} => {} ({})\n"
                         .format(mypid, orig_url, canon, fmt_status(resp)))
    else:
        sys.stderr.write("{:05}: {} => {}\n".format(mypid, orig_url, canon))
    sys.stderr.flush()

def log_fail(orig_url, resp):
    if not options.verbose: return
    sys.stderr.write("{:05}: {} => {}\n"
                     .format(mypid, orig_url, fmt_status(resp)))
    sys.stderr.flush()


def log_good_redirect(orig_url, redir, resp, cookies):
    if not options.verbose: return
    sys.stderr.write("{:05}: {} => {} to {}{}\n"
                     .format(mypid, orig_url, fmt_status(resp), redir,
                             fmt_cookies(cookies)))
    sys.stderr.flush()

def log_bad_redirect(orig_url, resp):
    if not options.verbose: return
    sys.stderr.write("{:05}: {} => {} to nowhere\n"
                     .format(mypid, orig_url, fmt_status(resp)))
    sys.stderr.flush()

def log_redirect_loop(orig_url, redir, resp):
    if not options.verbose: return
    sys.stderr.write("{:05}: {} => {} to {}, loop detected\n"
                     .format(mypid, orig_url, fmt_status(resp), redir))
    sys.stderr.flush()

def log_declined_redirect(orig_url, canon, redir, resp):
    if not options.verbose: return
    sys.stderr.write("{:05}: {} => {} ({} to {})\n"
                     .format(mypid, orig_url, canon, fmt_status(resp), redir))
    sys.stderr.flush()

def log_env_error(orig_url, exc):
    if not options.verbose: return
    if exc.filename:
        sys.stderr.write("{:05}: {} => {}: {}\n"
                         .format(mypid, orig_url, exc.filename, exc.strerror))
    elif exc.strerror:
        sys.stderr.write("{:05}: {} => {}\n"
                         .format(mypid, orig_url, exc.strerror))
    else:
        sys.stderr.write("{:05}: {} => {}\n"
                         .format(mypid, orig_url, str(exc)))
    sys.stderr.flush()

def log_http_error(orig_url, exc):
    if not options.verbose: return
    sys.stderr.write("{:05}: {} => HTTP error ({}): {}\n"
                     .format(mypid, orig_url, exc.__class__.__name__, str(exc)))
    sys.stderr.flush()

def log_gen_error(orig_url, exc):
    if not options.verbose: return
    sys.stderr.write("{:05}: {} => {}\n"
                     .format(mypid, orig_url, str(exc)))
    sys.stderr.flush()


# Custom canonization function which treats "foo.com/blah" as equivalent to
# "http://foo.com/blah" rather than as a partial URL with no host or scheme
# (as urlparse.urlsplit does).
def precanonize(url):
    (scheme, sep, rest) = url.partition('://')
    if sep == '':
        rest = scheme
        scheme = 'http'
    else:
        scheme = scheme.lower()
    (host, sep, path) = rest.partition('/')
    if path == '':
        path = '/'
    else:
        path = '/' + path
    host = host.lower()
    return scheme + "://" + host + path

# Custom canonization function which forces "http://foo.com" to
# "http://foo.com/" and removes empty params/query/fragment.
def postcanonize(url):
    (scheme, netloc, path, params, query, fragment) = \
        urlparse.urlparse(url)
    if (scheme == 'http' or scheme == 'https') and path == '':
        path = '/'
    norm = urlparse.urlunparse((scheme, netloc, path, params, query, fragment))
    # %-quote non-ASCII characters (and only those characters).
    return urllib.quote(norm, safe=string.punctuation)

# Subroutine of chase_redirects: report an anomalous response to the
# database-monger process.
def anomalous_response(orig_id, resp, body):

    # Response headers:
    headers = "\n".join("{}: {}".format(*kv)
                        for kv in sorted(resp.getheaders()))

    # The headers do not include the status line.
    status = fmt_status(resp)
    full_status = "HTTP/{} ".format(resp.version/10.) + status

    return (orig_id,
            None,
            status,
            full_status + "\n" + headers + "\n\n" + body)

def chase_redirects(args):
    global mypid
    orig_id, orig_url = args
    seen = set()
    cookies = cookielib.CookieJar()
    orig_url = orig_url.strip()
    url = precanonize(orig_url)
    log_start(orig_url)
    try:
        while True:
            req = HTTPRequest(url)
            url = req.get_full_url()

            seen.add(url)
            cookies.add_cookie_header(req)
            resp = req.fire()
            body = resp.read()

            # Only code 200 counts as success.
            if resp.status == 200:
                url = postcanonize(url)
                log_success(orig_url, url, resp)
                return (orig_id, url, "200 OK", None)

            # Codes 400, 401, 403, 404, and 410 are "normal" failures;
            # they do not get recorded as anomalous.
            if resp.status in (400, 401, 403, 404, 410):
                log_fail(orig_url, resp)
                return (orig_id, None, fmt_status(resp), None)

            # Codes 301, 302, 303, 307, 308 are understood as
            # redirection directives.  All other codes are treated as
            # anomalous failures.
            if resp.status not in (301, 302, 303, 307, 308):
                log_fail(orig_url, resp)
                return anomalous_response(orig_id, resp, body)

            # Redirected, so where to?
            location = resp.getheader("Location")
            if location is None:
                location = resp.getheader("Uri")
            if location is None:
                # A redirection with no Location or Uri header is anomalous.
                log_bad_redirect(orig_url, resp)
                return anomalous_response(orig_id, resp, body)

            # pick up any cookies attached to the redirection
            cookies.extract_cookies(resp, req)

            # update the url
            newurl = urlparse.urljoin(url, location)
            if newurl in seen:
                # Redirect loops are also anomalous.
                log_redirect_loop(orig_url, newurl, resp)
                return anomalous_response(orig_id, resp, body)

            log_good_redirect(orig_url, newurl, resp, cookies)
            url = newurl
            # and loop

    # Do not allow any exceptions to escape, or the entire job will crash.
    # Exceptions do _not_ count as anomalous; they generally occur before
    # the response is even received and there's nothing more to record.
    except httplib.HTTPException, e:
        log_http_error(orig_url, e)
        return (orig_id, None,
                "HTTP error ({}): {}".format(e.__class__.__name__,
                                             str(e)), None)

    # socket.timeout is an EnvironmentError, but its .strerror is None!
    except socket.timeout, e:
        log_gen_error(orig_url, e)
        return (orig_id, None, str(e), None)

    except EnvironmentError, e:
        log_env_error(orig_url, e)
        if e.filename: msg = "{}: {}".format(e.filename, e.strerror)
        elif e.strerror: msg = e.strerror
        else: msg = str(e)
        return (orig_id, None, msg, None)

    except Exception, e:
        log_gen_error(orig_url, e)
        return (orig_id, None, str(e), None)

# Bizarrely, sqlite3.Cursor is not an iterator nor does it have a method
# to return an iterator over the current query result.
def fetch_iter(cursor):
    while True:
        rows = cursor.fetchmany()
        if not rows: break
        for row in rows: yield row

# So we can stop cleanly on ^C.
interrupted = False
def sigint_handler(signo, frame):
    global interrupted
    interrupted = True

def worker_init():
    global mypid
    mypid = os.getpid()
    # the main process will issue a clean teardown on SIGINT
    signal.signal(signal.SIGINT, signal.SIG_IGN)

def sanitize_urls(db_filename):
    global interrupted

    # FIXME: Use urldb.py.
    db = sqlite3.connect(db_filename)
    cr = db.cursor()
    cr.executescript(r"""
PRAGMA encoding = "UTF-8";
PRAGMA foreign_keys = ON;
PRAGMA locking_mode = NORMAL;
""")

    # Cache the URL and status tables in memory.
    cr.execute("SELECT id, url FROM url_strings;")
    url_strings = { row[1]: row[0]
                    for row in fetch_iter(cr) }
    cr.execute("SELECT id, status FROM canon_statuses;")
    canon_statuses = { row[1]: row[0]
                       for row in fetch_iter(cr) }
    status_ok = canon_statuses["200 OK"]

    # Add rows to the canon_urls table for new entries in the urls table.
    cr.execute("INSERT INTO canon_urls"
               "  SELECT DISTINCT url, NULL, NULL FROM urls"
               "    WHERE url NOT IN (SELECT url FROM canon_urls);")

    # Load the list of URLs-to-do.
    # This must be done in advance so that multiprocessing doesn't try to
    # call the cursor on the wrong thread.
    cr.execute("SELECT u.url, v.url"
               "  FROM canon_urls as u"
               "  LEFT JOIN url_strings as v on u.url = v.id"
               "  WHERE u.canon IS NULL AND u.status IS NULL;")
    urls_to_process = [(row[0], row[1])
                        for row in fetch_iter(cr)]

    # We are now ready to spin up the worker pool.
    workers = multiprocessing.Pool(options.parallel, worker_init)
    pid = os.getpid()
    signal.signal(signal.SIGINT, sigint_handler)

    processed = 0
    total = len(urls_to_process)
    width = len(str(total))
    try:
        if not options.verbose:
            sys.stderr.write("\rprocessed {1:>{0}} of {2:>{0}}..."
                             .format(width, processed, total))
        cr.execute("BEGIN EXCLUSIVE;")
        for (orig_id, canon_url, status, anomaly) in \
                workers.imap_unordered(chase_redirects,
                                       urls_to_process,
                                       50):
            processed += 1
            if processed % 100 == 0:
                if not options.verbose:
                    sys.stderr.write("\rprocessed {1:>{0}} of {2:>{0}}..."
                                     .format(width, processed, total))
                db.commit()
                cr.execute("BEGIN EXCLUSIVE;")

            status_id = canon_statuses.get(status)
            if status_id is None:
                cr.execute("INSERT INTO canon_statuses VALUES(NULL, ?)",
                           (status,))
                status_id = cr.lastrowid
                canon_statuses[status] = status_id

            if anomaly is not None:
                cr.execute("INSERT INTO anomalies VALUES(?, ?, ?)",
                           (url_id, status_id, anomaly))

            if canon_url is None:
                canon_id = None
            else:
                canon_id = url_strings.get(canon_url)
                if canon_id is None:
                    cr.execute("INSERT INTO url_strings VALUES(NULL, ?)",
                               (canon_url,))
                    canon_id = cr.lastrowid
                    url_strings[canon_url] = canon_id

            cr.execute("UPDATE canon_urls SET canon = ?, status = ?"
                       "  WHERE url = ?",
                       (canon_id, status_id, orig_id))

            if interrupted:
                break

        if not options.verbose:
            sys.stderr.write("\rprocessed {1:>{0}} of {2:>{0}}...\n"
                             .format(width, processed, total))
        db.commit()
        workers.close()
        workers.join()

    except:
        if not options.verbose:
            sys.stderr.write("\n")
        db.rollback()
        workers.close()
        workers.join()
        raise

if __name__ == '__main__':

    op = optparse.OptionParser(
        usage="usage: %prog [options] database",
        version="%prog 1.0")
    op.add_option("-q", "--quiet",
                  action="store_false", dest="verbose", default=True,
                  help="don't print progress messages to stderr")
    op.add_option("-p", "--parallel",
                  action="store", dest="parallel", type="int", default=10,
                  help="number of simultaneous HTTP requests to issue")

    (options, args) = op.parse_args()
    sanitize_urls(args[0])
